# Robots.txt SEO Configuration Guide

## Overview

This document explains the robots.txt configuration for the Patinetas El√©ctricas Colombia website, designed to optimize search engine crawling and indexing while protecting sensitive areas.

## Configuration Location

- **File**: `src/app/robots.ts` (Next.js App Router)
- **Generated URL**: `/robots.txt`
- **Environment Variable**: `NEXT_PUBLIC_SITE_URL`

## Current Configuration

### Allowed Paths (Public Content)

The following paths are explicitly allowed for all search engines:

- `/` - Home page with FAQ section
- `/directorio` - Business directory
- `/directorio/*` - Individual business listings
- `/catalogo` - Electric scooter catalog
- `/catalogo/*` - Brand and model pages
- `/categorias` - Business categories
- `/categorias/*` - Category-specific listings
- `/ciudades` - City listings
- `/ciudades/*` - City-specific business listings
- `/negocio/*` - Individual business detail pages
- `/buscar` - Search functionality
- `/contacto` - Contact page
- `/acerca` - About page
- `/guia` - User guide
- `/faq` - FAQ section
- `/terminos` - Terms of service
- `/privacidad` - Privacy policy
- `/ayuda` - Help center
- `/agregar-negocio` - Add business form

### Blocked Paths (Protected Content)

The following paths are blocked from search engine crawling:

- `/admin` - Admin dashboard
- `/admin/*` - All admin functionality
- `/api/` - API endpoints
- `/api/*` - All API routes
- `/_next/` - Next.js internal files
- `/_next/*` - Build artifacts and static files
- `/private/` - Private content
- `/private/*` - Internal/private pages
- `/temp/` - Temporary files
- `/temp/*` - Development/testing content
- `/test/` - Test pages
- `/test/*` - Testing environment

### Pagination Limits

To prevent infinite crawling of paginated content:

- `/directorio/p/[5-9][0-9]*` - Blocks pages 50+
- `/directorio/p/[1-9][0-9][0-9]*` - Blocks pages 100+

### Search Parameter Limits

- `/buscar?*&*&*` - Blocks complex search URLs with multiple parameters

## Search Engine Specific Rules

### Googlebot (Most Permissive)
- **Crawl Delay**: None (default)
- **Access**: All public content allowed
- **Restrictions**: Only admin, API, and internal paths blocked

### Bingbot
- **Crawl Delay**: 2 seconds
- **Access**: Core public content allowed
- **Restrictions**: Same as Googlebot but with rate limiting

### All Other Bots
- **Crawl Delay**: 1 second
- **Access**: All public content with pagination limits
- **Restrictions**: More conservative approach with additional blocks

## Environment Configuration

### Development
```env
# Uses production URL as fallback
NEXT_PUBLIC_SITE_URL=https://patinetaelectrica.com.co
```

### Production
```env
# Set your production domain
NEXT_PUBLIC_SITE_URL=https://patinetaelectrica.com.co
```

## Sitemap Integration

The robots.txt automatically references the XML sitemap:
- **URL**: `{SITE_URL}/sitemap.xml`
- **Generated by**: `src/app/sitemap.ts`
- **Content**: All public pages with proper priorities and change frequencies

## SEO Benefits

### 1. **Improved Crawl Efficiency**
- Directs search engines to important content
- Prevents wasting crawl budget on admin/API pages
- Sets appropriate crawl delays to avoid server overload

### 2. **Enhanced Security**
- Blocks admin areas from appearing in search results
- Protects API endpoints from discovery
- Prevents indexing of development/test content

### 3. **Better User Experience**
- Ensures only relevant pages appear in search results
- Prevents users from finding admin or error pages
- Focuses search engine attention on valuable content

### 4. **Structured Data Integration**
- Works with FAQ structured data for rich snippets
- Supports business directory structured data
- Enables proper indexing of catalog pages

## Monitoring and Maintenance

### Google Search Console
1. Submit sitemap URL: `{SITE_URL}/sitemap.xml`
2. Monitor crawl errors and blocked resources
3. Check robots.txt testing tool for validation

### Regular Updates
- Review blocked paths when adding new features
- Update allowed paths for new public content
- Adjust crawl delays based on server performance

### Testing
- Test robots.txt at: `{SITE_URL}/robots.txt`
- Validate with Google Search Console robots.txt tester
- Monitor server logs for crawler activity

## Best Practices Implemented

1. **Specific User-Agent Rules**: Different rules for different search engines
2. **Crawl Delay Management**: Prevents server overload
3. **Wildcard Usage**: Efficient path blocking with `/*` patterns
4. **Sitemap Reference**: Helps search engines discover all pages
5. **Host Declaration**: Specifies canonical domain
6. **Environment Flexibility**: Works in development and production

## Troubleshooting

### Common Issues

1. **Pages Not Being Indexed**
   - Check if path is in disallow list
   - Verify sitemap includes the page
   - Test with Google Search Console

2. **Admin Pages in Search Results**
   - Confirm `/admin/*` is in disallow list
   - Check for meta robots tags on admin pages
   - Use Google Search Console to remove URLs

3. **High Server Load from Crawlers**
   - Increase crawl delay values
   - Monitor server logs for excessive requests
   - Consider blocking specific user agents if needed

### Validation Tools

- Google Search Console robots.txt tester
- Bing Webmaster Tools robots.txt validator
- Online robots.txt validators
- Server access logs analysis

## Future Considerations

- Monitor crawl budget usage in Search Console
- Adjust rules based on new content types
- Consider more granular user-agent rules if needed
- Update environment variables for production deployment
